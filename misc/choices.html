<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
  <title>choices</title>
  	<link rel="stylesheet" href="../styles/mystyles.css">
  </head>

<body>
    <img src="../images/csg4ed-small.png" alt="Enhancing Social Good in Computing"
    <!-- Site navigation menu -->

    <ul class="navbar">
        <li><a href="../index.html">Home page</a>
        <li><a href="topic.html">Technology/Topic</a>
        <li><a href="opportunities.html">Opportunities</a>
        <li><a href="risks.html">Risks</a>
        <li><a href="choices.html">Choices</a>
        <li><a href="ethics.html">Ethical Reflections</a>
        <li><a href="references.html">References</a>
        <li><a href="process.html">Process Support</a>
    </ul>

    <!-- Main content -->
    <h1>Technology Choices</h1>
<p>
    When thinking about our choices with Artificial Intelligence, the choices we have are essentially only limited by our current capabilities.<br>
    We can develop deep learning systems that allows the systems to collaborate with each other, mimicking the human learning.<br />
    We can develop machine learning systems to require a smaller number of examples to train the machine learning algorithm.<br /><br />

    “Misaligned or confused and conflated goals of an AI are going to be a significant concern of the future, as the AI will be extremely good at achieving its goals, but those goals may not represent what we wanted.”<br /><br />

    One choice we must make is to develop the machine learning systems to where an AI can explain the logic behind their judgement. 
    By doing so we then become able to understand where the algorithm fails and correct this. According to Baecker Ronald M. (2019), Computer scientists Jenna Burell and Ed 
    Felton have noted several reasons as to why an algorithm may not be explainable, one of these reasons being that the logic may not be justifiable. This is often referred 
    to as the "Black Box" problem (Kuang C. (2017)). Also mentioned in the article written by Kuang, is the idea that in order for an AI to be able to defend it's reasonings,
    it needs more AI. The example used, was allowing an AI to teach itself how to describe a picture. The AI would have two deep neural networks, one for recognizing images 
    and another for translating languages. They then used thousands of images with captions as the training samples. The image recognition network would process the images 
    while the second network would slowly become to associate certain words and images. Then with these networks, there would be another network dedicated to narrating this 
    process, allowing the AI to explain and justify it's conclusions. This example of visualizing explanations is also explained in more detail by Hendricks (2016). 
    This method works for simple problems. However, if we can refine it enough to work for complex problems we face today, it would be extremely useful in increasing our 
    trust in AI made decisions.<br/><br/>
    
    Another choice for development would be to create deep learning systems that can collaborate with each other. This stems from the fact that to create a deep learning system, there would be large
    amounts of data needed, and some companies don't have access to such large quantities required. To overcome this, it would be more effective and efficient for our 
    systems to require a smaller set of samples in order to train the algorithms required for a task. One way of incorporating the smaller data sets, would be for deep
    learning systems to pass on certain bits of information, referred to by Vincent (2016), as progressive neural networks. The only drawback noted in a paper by Hadsell (2016),
    is that, the more systems are chained together, the harder it becomes to control, and that is when the tasks performed are similar. Doing multiple vastly different tasks
    would make it near impossible to influence. If we can develop the deep learning systems to where the collaboration between multiple systems can still be influenced, and 
    we can still trace the justification of the systems logic, it would open a gate to a plethora of opportunities, as we would be able to create an AI that has a more human-like
    intelligence. <br/><br/>

    The final choice for development I want to mention is in regards to becoming more human-like. An AI does not currently have empathy, whether that's a good thing or not is up
    for debate. Epley (2007), states that AI can bridge social connection when there is insufficient human connection. Newman (2014) mentions that an AI capable of
    speech recognition and response is good for someone who finds it hard to interact with others, as it can provide good practice saying what you mean. For example, Siri will
    recognize your speech and respond accordingly. Since she is designed to be kind, if you were to say something in a negative manner, Siri would correct you but not in a way
    that is harsh, like other humans might, but in a way that would allow you to recognize ad correct the mistake on your own. However, since Siri cannot detect emotions, it can
    sometimes lead to weird responses based only on the speech recognized, which can also sometimes be inaccurate. By developing an AI to better detect emotions from tone in speech,
    we could further assist in the social development of humans who struggle with social interaction.

    
    <address>
        Made 24 May 2021<br>
        by Jade Thompson-Tavai.
    </address>

</html>